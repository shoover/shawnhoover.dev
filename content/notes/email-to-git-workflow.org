#+title: Posting to a Static Site by Email with GitHub Actions
#+description: A Posterous-like, serverless workflow to create plain text blog posts from emails
#+date: <2023-11-12 Sun 10:44>
#+setupfile: org.txt

In [[file:email-still-works.org][Email Still Works]] I mentioned I wrote some code to set up posting by email
for this site. This post shows how that works.

** Overview
The basic requirement is to fetch email from a dedicated mailbox, save each new
message to a post file, and push to git. I'm already using GitHub Actions for
deployment, so I chose that as the runtime environment.

To process new posts as quickly as possible (rather than on a cron schedule), I
also deployed a Cloudflare Email Worker to ping GitHub and trigger email
ingestion as soon as an email lands. In a few seconds after hitting send from
the mail app on my phone, the email is processed to orgmode and pushed to a PR.
From there I can preview the post in GitHub web's rendering of the .org file or
jump over to the freshly deployed staging site. Merging the PR deploys the post
to the main site.

Here's the end-to-end flow:

#+CAPTION: End-to-end email ingestion diagram
[[../assets/images/notes/email-to-git-workflow/end-to-end.png]]

** GitHub Actions Code
The GitHub workflow code is in [[https://github.com/shoover/shawnhoover.dev/blob/main/script/ingest-email.py][ingest-email.py]]. I'll walk through some of it
here.

Email fetching requires straightforward IMAP processing.

#+begin_src python
import dkim
import email
import imaplib

imap = imaplib.IMAP4_SSL(imap_server)
imap.login(imap_username, imap_password)
imap.select(imap_folder)

status, email_ids = imap.search(None, "UNSEEN")
email_id_list = email_ids[0].split()
for email_id in email_id_list:
    # Fetch with PEEK to keep the message unread pending successful processing.
    status, email_data = imap.fetch(email_id, "(BODY.PEEK[])")
    if status != "OK":
        raise Exception(f"Fetch {email_id} failed: {status}.")

    msg_bytes = email_data[0][1]

    # Scream if a message shows up failing ARC authentication.
    if not arc_authenticated(msg_bytes):
        raise Exception(f": {sender} ({subject})")

    post = extract_post(email.message_from_bytes(msg_bytes), org_dest=notes_dest, img_dest=img_dest)

    check_in(post['subject'], notes_dest, img_dest)

    imap.store(email_id, "+FLAGS", "\Seen")

imap.logout()
#+end_src


The happy path extracts the body from a multipart email (in theory plain text
email is handled--I didn't test it), does a little HTML cleanup, and converts to
orgmode using the venerable, amazing pandoc.

#+begin_src python
for part in msg.walk():
    # ... validate headers and content type

    if content_type == "text/html":
        html = BeautifulSoup(payload.decode(), 'html.parser')
        for br in html.find_all('br'):
            br.extract()
        content = subprocess.run(
            ["pandoc", "--from=html", "--to=org"],
            input=str(html),
            stdout=subprocess.PIPE,
            text=True,
        ).stdout
        
    # ... handle plain text
#+end_src

A separate walk (not shown) extracts image parts to disk in the GitHub runner
workspace. After rewriting image src attributes from =cid= references to
relative paths, post content is written to disk in a basic orgmode template.

#+begin_src python
image_link_base = pathlib.Path('..').joinpath(image_path.relative_to(image_path.parts[0]))
for img in images:
    org_link = f"file:{image_link_base}/{img['filename']}"
    content = content.replace(f"cid:{img['content_id']}", org_link)

# Create post file (orgmode template)
note_file_path = os.path.join(org_dest, f"{slug(subject)}.org")
with open(note_file_path, "w") as f:
    post_timestamp = pendulum.now().format('YYYY-MM-DD ddd HH:mm')
    f.write(f"""#+title: {subject}
#+date: <{post_timestamp}>
#+setupfile: org.txt

{content}""")
#+end_src

Shell commands push the files to git and create a PR. Lastly, the existing
deploy workflow is triggered to build the branch[fn:1].

** Email Worker Code
The GitHub job is triggered by a Cloudflare Email Worker. This part adds
operational overhead and could be replaced with a cron trigger, but it's worth
it for now to get immediate feedback.

The code does some sender validation and ultimately makes one GitHub API call:

#+begin_src js
const url = `https://api.github.com/repos/${owner_repo}/dispatches`;
const response = await fetch(url, {
    method: 'POST',
	headers: {
		"Authorization": `Bearer ${token}`,
		"X-GitHub-Api-Version": "2022-11-28",
		"Content-Type": "application/json;charset=UTF-8",
		"Accept": "application/vnd.github+json",
		"User-Agent": `${owner_repo} email-worker`,
	},
	body: JSON.stringify({
		"event_type": "email_received",
		"client_payload": {}
	}),
});
#+end_src

* Conclusion
After a bit of setup, this workflow is easy to use and so far it's working
great. With the flexibility of HTML converters and different attachment types,
the same concept could be extended to any static content format.

* Footnotes

[fn:1] This requirement to explicitly trigger the deploy job confused me. I
assumed the existing =push= trigger would happen automatically, and it didn't.
Once I learned GitHub does not trigger jobs from git operations made by existing
jobs, as a measure to prevent cycles, it was straightforward to make the deploy
job reusable and trigger it explicitly.
